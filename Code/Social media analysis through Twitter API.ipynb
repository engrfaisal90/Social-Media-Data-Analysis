{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Analysis through the Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This detailed video course is available at  https://codingwithmax.teachable.com/courses/ . The course name is \"Conquer Social Media through the Twitter API\". \n",
    "\n",
    "Note:coded in python 3.0.\n",
    "Remember to run it step-by-step. As the tutorial is in one flow.\n",
    "In every step some of the code is changed. To track the change code look for the comment \"###change####\" and then follow below that line.\n",
    "\n",
    "This is basic level tutorial for those we want some analysis on twitter data. this will give you the practical idea of how can you play with different API's to extract your data from twitter.\n",
    "This tutorial covers below lectures:\n",
    "\n",
    "1) sending first request to Twitter API\n",
    "\n",
    "2) Getting Tweets from Specific time\n",
    "\n",
    "3) Getting all tweets by moving backward in time\n",
    "\n",
    "4) Filtering for english tweets and picking keywords\n",
    "\n",
    "5) Identifying relevant tweets and keeping track of data\n",
    "\n",
    "6) Plotting the mined data from twitter\n",
    "\n",
    "7) Adjusting the maximum time and adding ticks to the graph\n",
    "\n",
    "8) Streaming live twitter data\n",
    "\n",
    "These libraries required in this tutorial can be found at these links.\n",
    "\n",
    "http://docs.python-requests.org/en/master/\n",
    "\n",
    "https://github.com/requests/requests-oauthlib\n",
    "\n",
    "https://github.com/bear/python-twitter\n",
    "\n",
    "To know more about installing python packages Please click on below link\n",
    "\n",
    "https://packaging.python.org/tutorials/installing-packages/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending first request to Twitter API\n",
    "First we will import the required libraries. Then we will make our URL query for sending it to the user. Authentication is required to access server and forward your query so that the server can give you back something. Four types of secret codes are required for authentication process. These secret codes can be obtained by creating app on https://apps.twitter.com/. To create app on twitter click on the link. These secret code can be obtained by first logging in to your twitter account.\n",
    "After getting those codes we set make our authention variable with the help of request_oauth library. Both URL and authentication is passed as request to server.if everything goes well the server respond to us and gives us the information requested against the  query in the URL. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#allow us to send HTTP msg to twitter api\n",
    "import requests\n",
    "\n",
    "#allow us to send the authentication with the request\n",
    "import requests_oauthlib as ra\n",
    "\n",
    "#here in query variable we are setting what we want to send to server\n",
    "query=\"earthquake\"\n",
    "#looking for the tweets contaning word stored in query\n",
    "URL= (\"https://api.twitter.com/1.1/search/tweets.json?q=\"+query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we get these credentials when we create an app on twitter\n",
    "twitter_consumer_key=\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXF\"\n",
    "twitter_consumer_secret=\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "\n",
    "twitter_access_token=\"XXXXXXX_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "twitter_access_secret=\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "\n",
    "\n",
    "#setting our authentication to pass it with the request\n",
    "authen= ra.OAuth1(twitter_consumer_key,twitter_consumer_secret,twitter_access_token,twitter_access_secret)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request allow us to send HTTP request. we send get request which contain URL and authentication.\n",
    "so here we are sending query of \"earthquake\".\n",
    "After sendng the request the twitter will give us back the latest tweets containing keyword \"earthquake\"\n",
    "if the print statement gives response 200 it means everthing is perfectly working. if response is 401 it means something is wrong with authentication keys. basically 400 response means something is wrong on your side. 500 response means something is wrong on server side. To know more about the response click on this link\n",
    "http://www.restapitutorial.com/httpstatuscodes.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backresponse=requests.get(URL,auth=authen)\n",
    "\n",
    "print(backresponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Data Recieved in Response\n",
    "Here we will discuss that what kind of data we get back in response and how can we parse the data to get our required data.\n",
    "\n",
    "The response which we get is in JSON format. lets print it and have a look at it.\n",
    "it contains alot of information including the tweet text, time, profile information, hashtag used, retweet information etc.\n",
    "the data is in form of dictionary and arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(backresponse.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of tweets to retrieve\n",
    "Keep in mind that we can extract upto 100 in single get request.Above data may contains information of multiple tweets. lets retrieve only one tweet so that we can futher study what type of information it contains. To notify the number of tweets we want to retrive in single request we will send parameter \"count\" in our URL to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"earthquake\"\n",
    "\n",
    "####changes#####\n",
    "\n",
    "#the count variable will control the number if tweets we want to retieve, here we will set it to 1.\n",
    "count=\"1\"\n",
    "#looking for sigle tweet contaning word stored in query\n",
    "URL= (\"https://api.twitter.com/1.1/search/tweets.json?q=\"+query+\"&count=\"+count)\n",
    "\n",
    "###below is the same code which we have earlier used#####\n",
    "\n",
    "authen= ra.OAuth1(twitter_consumer_key,twitter_consumer_secret,twitter_access_token,twitter_access_secret)\n",
    "backresponse=requests.get(URL,auth=authen)\n",
    "\n",
    "print(backresponse.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting tweet text and other information\n",
    "Now we will go deep into dictionary and extract the text of tweet. we can extract any type of information related to this tweet. Here we will extraxt the tweet text, hastags and id of the tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######below is the same code which we have earlier used#####\n",
    "\n",
    "query=\"earthquake\"\n",
    "count=\"1\"\n",
    "URL= (\"https://api.twitter.com/1.1/search/tweets.json?q=\"+query+\"&count=\"+count)\n",
    "\n",
    "authen= ra.OAuth1(twitter_consumer_key,twitter_consumer_secret,twitter_access_token,twitter_access_secret)\n",
    "backresponse=requests.get(URL,auth=authen)\n",
    "tweetInformation=backresponse.json()\n",
    "\n",
    "###changes###\n",
    "\n",
    "###getting Tweet####\n",
    "# we get Array against the key \"statuses\", then in that array we extracted the dictionary at zero index and then further\n",
    "#in that dictionary we searched for value against the key \"text\".\n",
    "tweet = tweetInformation[\"statuses\"][0][\"text\"]\n",
    "print(tweet)\n",
    "\n",
    "####Getting HashTags#####\n",
    "#to get hashtag we get Array against the key \"statuses\", then in that array we extracted the dictionary at zero index and then further\n",
    "#in that dictionary we searched for value against the key \"entities\" then further went deep to get array against key \"hashtags\" then \n",
    "#took the zero index of that Array and find against the key \"text\"\n",
    "hashtagsArray= tweetInformation[\"statuses\"][0][\"entities\"][\"hashtags\"]\n",
    "#to confirm whether there are hashtags or not we will further check whether the array contain something or its empty\n",
    "if (len(hashtagsArray)>0):\n",
    "    hastags=hashtagsArray[0][\"text\"]\n",
    "    print(hashtags)\n",
    "else:\n",
    "    print(\"No hastags found\")\n",
    "    \n",
    "### Getting Tweet id####\n",
    "#every tweet has its own unique id\n",
    "id=tweetInformation[\"statuses\"][0][\"id\"]\n",
    "\n",
    "print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet between specific interval of time\n",
    "we will insert \"since\" and \"until\" in the url to get tweet of specific time interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######below is the same code which we have earlier used#####\n",
    "query=\"eartquake\"\n",
    "count=\"1\"\n",
    "\n",
    "####changes######\n",
    "\n",
    "#in since variable we are storing the date from where exatcly we want to start\n",
    "since=\"2018-02-05\"\n",
    "#in untill variable we are storing ending date or the date untill we want to extract\n",
    "until=\"2018-02-08\"\n",
    "#Looking for tweets between the specified interval\n",
    "URL= (\"https://api.twitter.com/1.1/search/tweets.json?q=\"+query+\"&since=\"+since+\"&until=\"+until+\"&count=\"+count)\n",
    "\n",
    "#######below is the same code which we have earlier used#####\n",
    "\n",
    "authen= ra.OAuth1(twitter_consumer_key,twitter_consumer_secret,twitter_access_token,twitter_access_secret)\n",
    "backresponse=requests.get(URL,auth=authen)\n",
    "tweetInformation=backresponse.json()\n",
    "print(tweetInformation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we have already seen that max_id is unique id of every tweet. so can we get all the tweets before or after that id\n",
    "we will make changes to the url which we are sending in our request. we will include max_id parameter in the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######below is the same code which we have earlier used#####\n",
    "query=\"earthquake\"\n",
    "count=\"100\"\n",
    "since=\"2018-02-05\"\n",
    "\n",
    "####changes####\n",
    "\n",
    "#in max_id we are storing id of the ending tweet\n",
    "max_id='961335265041272832'\n",
    "#Looking for tweets between the starting time and maximum tweet id\n",
    "URL= (\"https://api.twitter.com/1.1/search/tweets.json?q=\"+query+\"&since=\"+since+\"&max_id=\"+max_id+\"&count=\"+count)\n",
    "\n",
    "#######below is the same code which we have earlier used#####\n",
    "\n",
    "authen= ra.OAuth1(twitter_consumer_key,twitter_consumer_secret,twitter_access_token,twitter_access_secret)\n",
    "backresponse=requests.get(URL,auth=authen)\n",
    "tweetInformation=backresponse.json()\n",
    "print(tweetInformation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting all tweets by moving backward in time\n",
    "As we have seen above that we can extract only 100 tweets in one request. Now we want to change our URL information dynamically so that we can extract all the tweets in a time frame. we will get the id of the last tweet in 100 tweet bundle and then again send a new request to server with this new information. In that way we will move backward in time to extract 100 tweets each time. The while loop will go on untill we have nill information response. Inside the while loop there is for loop. This for loop is used for unpacking the 100 tweets package and extract all the the 100 tweets from that json response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###below is the same code in earlier step###\n",
    "query=\"earthquake\"\n",
    "count=\"100\"\n",
    "since=\"2018-02-05\"\n",
    "max_id='961335265041272832'\n",
    "URL= (\"https://api.twitter.com/1.1/search/tweets.json?q=\"+query+\"&since=\"+since+\"&max_id=\"+max_id+\"&count=\"+count)\n",
    "\n",
    "authen= ra.OAuth1(twitter_consumer_key,twitter_consumer_secret,twitter_access_token,twitter_access_secret)\n",
    "backresponse=requests.get(URL,auth=authen)\n",
    "tweetInformation=backresponse.json()\n",
    "\n",
    "\n",
    "###changes###\n",
    "\n",
    "## the loop will go on untill we have nill information in json i.e no tweets\n",
    "while (len(tweetInformation)!=0):\n",
    "    #getting the information against key \"statuses\" from the json response\n",
    "    alltweets=tweetInformation[\"statuses\"]\n",
    "    #As we have set count to 100 so this loop will give us all the tweets we get in a single get request\n",
    "    for eachtweet in range(len(alltweets)):\n",
    "        #accessing tweets same as above \n",
    "        tweet = alltweets[eachtweet][\"text\"]\n",
    "        print(tweet)\n",
    "        #accessing id\n",
    "        max_id=alltweets[eachtweet][\"id_str\"]\n",
    "        print(max_id)\n",
    "    \n",
    "    #we want to print date and time of every 100th tweet    \n",
    "    print(alltweets[eachtweet][\"created_at\"])    \n",
    "    #dynamically changing our URL\n",
    "    URL= (\"https://api.twitter.com/1.1/search/tweets.json?q=\"+query+\"&since=\"+since+\"&max_id=\"+max_id+\"&count=\"+count)\n",
    "    #inserted the try and except block so that our program does not give an error, if their is something wrong it will break the\n",
    "    #loop and end the program.\n",
    "    try:\n",
    "        #sending get request with new id\n",
    "        backresponse=requests.get(URL,auth=authen)\n",
    "        #gettng the new json data of the new request\n",
    "        tweetInformation=backresponse.json()\n",
    "    except:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering language specific tweets and picking out keywords\n",
    "The json data which is returned to us contain language information. The key used for language is \"lang\". we can filter a language specific tweets. Here below we will filter out all english tweets.  More over we also look for specific keywords \"Taiwan\" in a tweet. An earthquake occured on 6 feb 2018 in taiwan. so we want to cover data of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###below is the same code in earlier step###\n",
    "query=\"earthquake\"\n",
    "count=\"100\"\n",
    "since=\"2018-02-05\"\n",
    "max_id='961335265041272832'\n",
    "URL= (\"https://api.twitter.com/1.1/search/tweets.json?q=\"+query+\"&since=\"+since+\"&max_id=\"+max_id+\"&count=\"+count)\n",
    "\n",
    "authen= ra.OAuth1(twitter_consumer_key,twitter_consumer_secret,twitter_access_token,twitter_access_secret)\n",
    "backresponse=requests.get(URL,auth=authen)\n",
    "tweetInformation=backresponse.json()\n",
    "\n",
    "####changes####\n",
    "#we will be looking for tweets having taiwan. so basically we want to find all those tweets in earthquake of taiwan is mentioned\n",
    "keywords=[\"Taiwan\",\"taiwan\"]\n",
    "\n",
    "\n",
    "###below is the same code in earlier step###\n",
    "while (len(tweetInformation)!=0):\n",
    "    alltweets=tweetInformation[\"statuses\"]\n",
    "    for eachtweet in range(len(alltweets)):\n",
    "        tweet = alltweets[eachtweet][\"text\"]\n",
    "        #print(tweet)\n",
    "        max_id=alltweets[eachtweet][\"id_str\"]\n",
    "        #print(max_id)\n",
    "        \n",
    "        \n",
    "        ###changes###\n",
    "        \n",
    "        #gettng language information\n",
    "        lang=alltweets[eachtweet][\"lang\"]\n",
    "        #filtering tweets with language english\n",
    "        if (lang==\"en\"):\n",
    "            #checking all the keywords\n",
    "            for word in keywords:\n",
    "                #extracting only those tweet containing taiwan.\n",
    "                if word in tweet:\n",
    "                    print(\"Filtered tweet : \" +tweet)\n",
    "                    #break out of the iterating keyword loop as keyword is found\n",
    "                    break\n",
    "                    \n",
    "            \n",
    "        \n",
    "    \n",
    "    ###below is the same code in earlier step###\n",
    "    \n",
    "    print(alltweets[eachtweet][\"created_at\"])    \n",
    "    URL= (\"https://api.twitter.com/1.1/search/tweets.json?q=\"+query+\"&since=\"+since+\"&max_id=\"+max_id+\"&count=\"+count)\n",
    "    try:\n",
    "        backresponse=requests.get(URL,auth=authen)\n",
    "        tweetInformation=backresponse.json()\n",
    "    except:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### identifying relevant tweets and keeping track of data\n",
    "Basically here we want to know how many tweets and which tweets has been done at a certain hour.For that we will make three dictionary variables to keep track of these. first variable \"tweet_count_hour\" is a dictionary of number of tweets in that hour. second variable \"tweet_hour\" is dictionary of all tweets in that hour. Third variable \"all_hour\" is an array of all the hours at which tweeets occured.\n",
    "As we want to keep track of every hour data so we will extract minute time of every tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###below is the same code in earlier step###\n",
    "query=\"earthquake\"\n",
    "count=\"100\"\n",
    "since=\"2018-02-05\"\n",
    "max_id='961335265041272832'\n",
    "URL= (\"https://api.twitter.com/1.1/search/tweets.json?q=\"+query+\"&since=\"+since+\"&max_id=\"+max_id+\"&count=\"+count)\n",
    "\n",
    "authen= ra.OAuth1(twitter_consumer_key,twitter_consumer_secret,twitter_access_token,twitter_access_secret)\n",
    "backresponse=requests.get(URL,auth=authen)\n",
    "tweetInformation=backresponse.json()\n",
    "\n",
    "####changes####\n",
    "#\"tweet_count_hour\" is a dictionary of number of tweets in each hour\n",
    "tweet_count_hour={}\n",
    "#\"tweet_hour\" is dictionary of all tweets in each hour\n",
    "tweet_hour={}\n",
    "#\"all_hour\" is an array of all the hour at which tweeets occured\n",
    "all_hour=[]\n",
    "\n",
    "###below is the same code in earlier step###\n",
    "keywords=[\"Taiwan\",\"taiwan\"]\n",
    "\n",
    "while (len(tweetInformation)!=0):\n",
    "    alltweets=tweetInformation[\"statuses\"]\n",
    "    for eachtweet in range(len(alltweets)):\n",
    "        tweet = alltweets[eachtweet][\"text\"]\n",
    "        #print(tweet)\n",
    "        max_id=alltweets[eachtweet][\"id_str\"]\n",
    "        #print(max_id)\n",
    "        \n",
    "        ###changes###\n",
    "        \n",
    "        #the date stamp is in format \"Thu Feb 08 15:59:00\" we dont need second, minute, day and month information so we will skip that. only first\n",
    "        #the indices [8:13] will be taken for date and hour information. \n",
    "        currentTime=alltweets[eachtweet][\"created_at\"][8:13]\n",
    "        \n",
    "        \n",
    "        ###below is the same code in earlier step###\n",
    "        \n",
    "        lang=alltweets[eachtweet][\"lang\"]\n",
    "        if (lang==\"en\"):\n",
    "            for word in keywords:\n",
    "                if word in tweet:\n",
    "                    if word in tweet:\n",
    "                        #print(\"Filtered tweet : \" +tweet)\n",
    "                                   \n",
    "                        ###changes###\n",
    "                        \n",
    "                        #if current time is in tweet_count_hour we will increment the value against that time. means that another tweet is\n",
    "                        #tweeted at that time. in this way we will group tweets by every hour\n",
    "                        if currentTime in tweet_count_hour:\n",
    "                            tweet_count_hour[currentTime]+=1 \n",
    "                            #if at the hour stamp key a tweet is already present and another tweet is found at same time so we \n",
    "                            #we will append it at that array.\n",
    "                            tweet_hour[currentTime].append(tweet)\n",
    "                        #if current time is not in tweet_count_hour it means that time is not in dictioary so we will add it\n",
    "                        else:\n",
    "                            tweet_count_hour[currentTime]=1\n",
    "                            #here we are saving the first tweet at that hour. the tweet is saved as array becuase may be more\n",
    "                            #than one tweet may have occured in that hour.\n",
    "                            tweet_hour[currentTime]=[tweet]\n",
    "                            all_hour.append(currentTime)\n",
    "                        #break out of the iterating keyword loop as keyword is found\n",
    "                        break\n",
    "                    \n",
    "    #here we print all the three variables and see whats in these variables\n",
    "    print(tweet_hour)\n",
    "    print(tweet_count_hour)\n",
    "    print(all_hour)\n",
    "    #inserting this break just to see only data only after 100 tweets and then stop it\n",
    "    break\n",
    "    \n",
    "            \n",
    "        \n",
    "    \n",
    "    ###below is the same code in earlier step###\n",
    "    \n",
    "    print(alltweets[eachtweet][\"created_at\"])    \n",
    "    URL= (\"https://api.twitter.com/1.1/search/tweets.json?q=\"+query+\"&since=\"+since+\"&max_id=\"+max_id+\"&count=\"+count)\n",
    "    try:\n",
    "        backresponse=requests.get(URL,auth=authen)\n",
    "        tweetInformation=backresponse.json()\n",
    "        alltweets=tweetInformation[\"statuses\"]\n",
    "    except:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the number of tweets against a counter\n",
    "at Feb 6, 11:50 PM earthquake occured in taiwan. we want to draw a graph that how many tweets occured about that earthquake in these these days. we need import for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "###below is the same code in earlier step###\n",
    "query=\"earthquake\"\n",
    "count=\"100\"\n",
    "since=\"2018-02-04\"\n",
    "max_id='961335265041272832'\n",
    "URL= (\"https://api.twitter.com/1.1/search/tweets.json?q=\"+query+\"&since=\"+since+\"&max_id=\"+max_id+\"&count=\"+count)\n",
    "\n",
    "authen= ra.OAuth1(twitter_consumer_key,twitter_consumer_secret,twitter_access_token,twitter_access_secret)\n",
    "backresponse=requests.get(URL,auth=authen)\n",
    "tweetInformation=backresponse.json()\n",
    "\n",
    "tweet_count_hour={}\n",
    "tweet_hour={}\n",
    "all_hour=[]\n",
    "keywords=[\"Taiwan\",\"taiwan\"]\n",
    "while (len(tweetInformation)!=0):\n",
    "    #the try and except is just if something went wrong\n",
    "    try:\n",
    "        alltweets=tweetInformation[\"statuses\"]\n",
    "    except:\n",
    "        print (\"No data retrieved Run again after 15 minutes\")\n",
    "        \n",
    "    for eachtweet in range(len(alltweets)):\n",
    "        tweet = alltweets[eachtweet][\"text\"]\n",
    "        #print(tweet)\n",
    "        max_id=alltweets[eachtweet][\"id_str\"]\n",
    "        #print(max_id)\n",
    "        currentTime=alltweets[eachtweet][\"created_at\"][9:13]\n",
    "        \n",
    "        lang=alltweets[eachtweet][\"lang\"]\n",
    "        if (lang==\"en\"):\n",
    "            for word in keywords:\n",
    "                if word in tweet:\n",
    "                    if word in tweet:\n",
    "                        #print(\"Filtered tweet : \" +tweet)     \n",
    "                        if currentTime in tweet_count_hour:\n",
    "                            tweet_count_hour[currentTime]+=1 \n",
    "                            tweet_hour[currentTime].append(tweet)\n",
    "                        else:\n",
    "                            tweet_count_hour[currentTime]=1\n",
    "                            tweet_hour[currentTime]=[tweet]\n",
    "                            all_hour.append(currentTime)\n",
    "                        break\n",
    "                    \n",
    "   \n",
    "    #print(tweet_hour)\n",
    "    #print(tweet_count_hour)\n",
    "    #print(all_hour)\n",
    "    \n",
    "    #print(alltweets[eachtweet][\"created_at\"])    \n",
    "    URL= (\"https://api.twitter.com/1.1/search/tweets.json?q=\"+query+\"&since=\"+since+\"&max_id=\"+max_id+\"&count=\"+count)\n",
    "    try:\n",
    "        backresponse=requests.get(URL,auth=authen)\n",
    "        tweetInformation=backresponse.json()\n",
    "        alltweets=tweetInformation[\"statuses\"]\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "  ###########   Changes   ###############\n",
    "\n",
    "#this variable is for saving all the tweets that occured in one hour into array\n",
    "tweet_counts=[]\n",
    "#these two variables are just for creating the x-axis ticks label from counter\n",
    "tickcount=0\n",
    "x_ax=[]\n",
    "#this variable will save orignal time so that we can assign it to ticks\n",
    "time_ticks=[]\n",
    "\n",
    "#the time will be appended in backward way\n",
    "for t in all_hour :\n",
    "    #appending the value against each hour key \n",
    "    tweet_counts.append(tweet_count_hour[t])\n",
    "    #for overwriting the xticks counter\n",
    "    time_ticks.append(t)\n",
    "     #appending counter to generate x-axis ticks\n",
    "    x_ax.append(tickcount)\n",
    "    tickcount+=1\n",
    "    \n",
    "plt.figure(figsize=(15,10))\n",
    "#plotting the number of tweets in one hour against counter value\n",
    "plt.plot(x_ax,  tweet_counts,label=\"Earthquake\", c=\"red\")\n",
    "#changing the x-axis counter ticks into corresponding time \n",
    "plt.xticks(x_ax,all_hour)\n",
    "#setting the figure size\n",
    "\n",
    "#setting a legend for the label window\n",
    "plt.legend()\n",
    "#showing the graph\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Data from Twitter\n",
    "we can get live data from twitter by using twitter-python api. the link is already given at the top. To do anything through Twitter api read the documentation http://python-twitter.readthedocs.io/en/latest/twitter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "#accessing the api class of twitter library. we pass our credentials for authentication\n",
    "api= twitter.Api(twitter_consumer_key,twitter_consumer_secret,twitter_access_token,twitter_access_secret)\n",
    "keywords=[\"Taiwan\",\"taiwan\",\"earthquake\",\"Earthquake\"]\n",
    "#the GetStreamFilter method of api class return public real time tweets. for more information read the api documentation.\n",
    "for eachtweet in api.GetStreamFilter(track=keywords):\n",
    "    print(eachtweet)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for getting the exact text we need to mention the \"text\" \n",
    "\n",
    "for more information consult the twitter api documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "api= twitter.Api(twitter_consumer_key,twitter_consumer_secret,twitter_access_token,twitter_access_secret)\n",
    "keywords=[\"Trump\",\"donald\"]\n",
    "for eachtweet in api.GetStreamFilter(track=keywords):\n",
    "    print(eachtweet[\"text\"])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
